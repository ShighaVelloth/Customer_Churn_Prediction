# Customer Churn Prediction 

# Model
DECISION TREE

Decision Tree is the most powerful and popular tool for classification and prediction. A Decision tree is a flowchart-like tree structure, where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (terminal node) holds a class label.

The strengths of decision tree methods are:

1.Decision trees are able to generate understandable rules.

 2.Decision trees perform classification without requiring much computation.
 
 3.Decision trees are able to handle both continuous and categorical variables. 
 
4.Decision trees provide a clear indication of which fields are most important for prediction or classification.

GINI INDEX

The internal working of Gini impurity is also somewhat similar to the working of entropy in the Decision Tree. In the Decision Tree algorithm, both are used for building the tree by splitting as per the appropriate features but there is quite a difference in the computation of both the methods.

RANDOM FOREST

Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned. Random decision forests correct for decision trees' habit of overfitting to their training set. Random forests generally outperform decision trees

